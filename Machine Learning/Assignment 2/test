def train_test_split(data, test_size=0.2):
    test_size = round(test_size * len(data))
    index = random.sample(population=list(range(len(data))), k=test_size)

    train_data = np.delete(data, index, axis=0)
    test_data = data[index]

    return train_data, test_data


def check_purity(data):
    unique_values = np.unique(data[:, -1])
    if len(unique_values) == 1:
        return True

    else:
        return False


def classify_data(data):
    unique_classes, counts_unique_classes = np.unique(data[:, -1], return_counts=True)
    i = counts_unique_classes.argmax()
    classification = int(unique_classes[i])

    return classification


def possible_splits(data):
    potential_split = {}
    _, cols = data.shape
    for col_index in range(cols - 1):
        potential_split[col_index] = []
        unique_val = np.unique(data[:, col_index])

        for index in range(len(unique_val)):
            if index != 0:
                current_value = unique_val[index]
                previous_value = unique_val[index - 1]
                potential_split[col_index].append((current_value + previous_value) / 2)

    return potential_split


def entropy_(data):
    _, counts = np.unique(data[:, -1], return_counts=True)
    size = len(data)
    # gini_parent = 1 - sum((count / size) ** 2 for count in counts)
    entropy = 0
    for count in counts:
        probability = count / size
        entropy += -probability * np.log2(probability)
    return entropy


def entropy_split(left_subtree, right_subtree, size):
    size_left = len(left_subtree) / size
    size_right = len(right_subtree) / size

    return size_left * entropy_(left_subtree) + size_right * entropy_(right_subtree)


def determine_best_split(data, potential_splits):
    best_entropy = entropy_(data)
    best_split_column, best_split_value = None, None
    info_gain = -9999

    for column_index in potential_splits:
        for value in potential_splits[column_index]:
            left_subtree, right_subtree = split_data(data, split_column=column_index, split_value=value)
            current_entropy = entropy_split(left_subtree, right_subtree, len(data))
            current_info_gain = best_entropy - current_entropy

            if current_info_gain >= info_gain:
                info_gain = current_info_gain
                best_split_column = column_index
                best_split_value = value

    return best_split_column, best_split_value, info_gain


def split_data(data, split_column, split_value):
    split_column_values = data[:, split_column]

    left_subtree = data[split_column_values <= split_value]
    right_subtree = data[split_column_values > split_value]

    return left_subtree, right_subtree


def classifier(data, features, counter=0, min_samples=10, max_depth=4):

    if check_purity(data) or len(data) < min_samples or counter == max_depth:
        classification = classify_data(data)

        return classification

    else:
        counter += 1

        potential_splits = possible_splits(data)
        split_column, split_value, info_gain = determine_best_split(data, potential_splits)

        if counter - 1 == 0:
            print("Root Node Information Gain = {:.3f}".format(info_gain))

        left_subtree, right_subtree = split_data(data, split_column, split_value)
        question = "{} <= {:.2f}".format(features[split_column], split_value)
        sub_tree = {question: {}}

        yes_answer = classifier(left_subtree, features, counter)
        no_answer = classifier(right_subtree, features, counter)

        if yes_answer == no_answer:
            sub_tree = int(yes_answer)

        else:
            sub_tree[question]['T'] = yes_answer
            sub_tree[question]['F'] = no_answer

        return sub_tree


def predict(row, tree, features):

    for node in tree.keys():
        label, comparator, value = node.split(" ")
        ind = features.index(label)

        if row[ind] <= float(value):
            answer = tree[node]['T']

        else:
            answer = tree[node]['F']

        if not isinstance(answer, dict):
            return answer

        else:
            answer = predict(row, answer, features)

    return answer


def accuracy(answers, test_data):
    correct = 0
    for i, j in zip(answers, test_data):
        if i == j:
            correct += 1

    accuracy = correct / len(test_data)

    return accuracy

# def draw(parent_name, child_name):
#     edge = pydot.Edge(parent_name, child_name)
#     graph.add_edge(edge)

# def visit(node, parent=None):
#     for k,v in node.items():
#         if isinstance(v, dict):
#             # We start with the root node whose parent is None
#             # we don't want to graph the None node
#             if parent:
#                 draw(parent, k)
#             visit(v, k)
#         else:
#             draw(parent, k)
#             # drawing the label using a distinct name
#             draw(str(k), str(k) +'_'+ str(v))


# if __name__ == '__main__':
import pandas as pd
import numpy as np
import random
from pprint import pprint
headers = ['Variance', 'Skewness', 'Curtosis', 'Entropy', 'Class']

data = pd.read_csv(r'C:\Users\nakul\Downloads\data_banknote_authentication.csv').values

train_data, test_data = train_test_split(data)
tree = classifier(train_data, headers)
pprint(tree)
ans = []

for row in test_data[:, :-1]:
    ans.append(predict(row, tree, headers))

acc = accuracy(ans, test_data[:, -1])
print("Accuracy = {:.2%}".format(acc))


def draw(parent_name, child_name):
    edge = pydot.Edge(parent_name, child_name)
    graph.add_edge(edge)


def visit(node, parent=None):
    for k, v in node.items():
        if isinstance(v, dict):
            # We start with the root node whose parent is None
            # we don't want to graph the None node
            if parent:
                draw(parent, k)
            visit(v, k)
        else:
            draw(parent, k)
            # drawing the label using a distinct name

            draw(k, k + '_' + str(v))

import pydot
graph = pydot.Dot(graph_type='graph')
visit(tree)
graph.write_png('example1_graph.png')
